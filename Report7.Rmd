---
title: "Statistical Inference"
subtitle: "2-way Anova"
author: "Your name here"
date: Published on `r format(Sys.time(), "%A %d %B %Y")`
output:
  rmdformats::readthedown:
    highlight: kate
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999, digits=3) 
library(rmdformats)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(datarium)
library(ggplot2)
library(knitr)
set.seed(123)
data("jobsatisfaction", package = "datarium")
library(report)
library(emmeans)
library(scorecard)
```

## 2-way Anova

Two-way ANOVA test hypotheses

-   There is no difference in the means of factor A

-   There is no difference in means of factor B

-   There is no interaction between factors A and B

The alternative hypothesis for cases 1 and 2 is: the means are not equal.

The alternative hypothesis for case 3 is: there is an interaction between A and B.

## Data

In this report we will the *jobsatisfaction* dataset [datarium package], which contains the job satisfaction score organized by gender and education levels.

In this study, a research wants to evaluate if there is a significant two-way interaction between gender and education_level on explaining the job satisfaction score.

An **interaction effect** occurs when the effect of one independent variable on an outcome variable depends on the level of the other independent variables. If an interaction effect does not exist, main effects could be reported.

## Descriptive statistics

Before running a model, you should always plot the data, to check that your assumptions look okay.

Here are a couple plots you might generate while analyzing these data:

```{r}
ggplot(jobsatisfaction, aes(x=score)) + 
  geom_histogram(bins=10) + 
  facet_grid(gender ~ education_level) + 
  theme_classic()
```

Boxplot, to highlight the group means:

```{r}
bxp<- ggboxplot(jobsatisfaction, y="score", x="gender", color ="education_level") 
bxp
```

The distributions within each cell look pretty wonky, but that's not particularly surprising given the small sample size (n=58):

```{r}
xtabs(~ gender + education_level, data = jobsatisfaction)
```

Compute the mean and the SD (standard deviation) of the score by groups:

``` {r}
jobsatisfaction %>%
  group_by(gender, education_level) %>%
  get_summary_stats(score, type = "mean_sd")
```

## Assumptions

### Outliers

Identify outliers in each cell design:

```{r}
jobsatisfaction %>%
  group_by(gender, education_level) %>%
  identify_outliers(score)
```

There were no extreme outliers.

### Normality

Check normality assumption by analyzing the model residuals. QQ plot and Shapiro-Wilk test of normality are used.

```{r}
jobsatisfaction %>%
  group_by(gender, education_level) %>%
  shapiro_test(score)
```

The score were normally distributed (p > 0.05) for each cell, as assessed by Shapiro-Wilk’s test of normality.

```{r message=FALSE, warning=FALSE}
ggqqplot(jobsatisfaction, "score") +
  facet_grid(gender~education_level)
```

All the points fall approximately along the reference line, for each cell. So we can assume normality of the data.

### Homogeneity of variance

This can be checked using the Levene's test:

```{r}
jobsatisfaction %>%
  levene_test(score~gender*education_level)
```

The p-value is > 0.05, which is not significant. This means that, there is not significant difference between variances across groups. Therefore, we can assume the homogeneity of variances in the different treatment groups. In a situation where the homogeneity of variance assumption is not met, you can compute the Welch one-way ANOVA test using the function welch_anova_test()[rstatix package]. This test does not require the assumption of equal variances.

The residuals versus fits plot can be used to check the homogeneity of variances:

```{r}
model<-lm(score~gender*education_level,data=jobsatisfaction)
plot(model, 1)
```

In the plot above, there is no evident relationships between residuals and fitted values (the mean of each groups), which is good. So, we can assume the homogeneity of variances.

## Anova

In this example, the effect of *education_level* is our focal variable, that is our primary concern.

It is thought that the effect of *education_level* will depend on one other factor, *gender*, which are called a **moderator variable**.

```{r}
results <- aov(data=jobsatisfaction,score~gender*education_level)
anova(results)
rep<-report(results)
```

`r rep`

We can also use the 'rstatix' package with the set of "_" functions that easily cooperate with pipes. In the R code below, the asterisk represents the interaction effect and the main effect of each variable (and all lower-order interactions).

``` {r}
results2 <- jobsatisfaction %>% anova_test(score ~ gender * education_level)
results2

```


From the above ANOVA table, it can be seen that there are significant differences between groups (p = 0.016), which are highlighted with “*“, F(2, 27) = 4.85, p = 0.16, eta2[g] = 0.26.

where:

-    F indicates that we are comparing to an F-distribution (F-test); (2, 27) indicates the degrees of freedom in the numerator (DFn) and the denominator (DFd), respectively; 4.85 indicates the obtained F-statistic value
-    p specifies the p-value
-    ges is the generalized effect size (amount of variability due to the factor)

## Post-hoc tests

A significant two-way interaction indicates that the impact that one factor (e.g., education_level) has on the outcome variable (e.g., job satisfaction score) depends on the level of the other factor (e.g., gender) (and vice versa). So, you can decompose a significant two-way interaction into:

-   Simple main effect: run one-way model of the first variable at each level of the second variable,

-   Simple pairwise comparisons: if the simple main effect is significant, run multiple pairwise comparisons to determine which groups are different.

For a non-significant two-way interaction, you need to determine whether you have any statistically significant main effects from the ANOVA output. A significant main effect can be followed up by pairwise comparisons between groups.

### Procedure for significant two-way interaction


#### Main effects

In our example, you could therefore investigate the effect of education_level at every level of gender or investigate the effect of gender at every level of the variable education_level.

Here, we’ll run a one-way ANOVA of education_level at each levels of gender.

If the two-way interaction is not statistically significant, you need to consult main effect for each of the two variables (gender and education_level) in the ANOVA output.

As we can see in the ANOVA table only main effects of education levels are statistically significant.

Here, we’ll run a one-way ANOVA of education_level at each levels of gender:

``` {r}
# Group the data by gender and fit  anova
model <- lm(score ~ gender * education_level, data = jobsatisfaction)
jobsatisfaction %>%
  group_by(gender) %>%
  anova_test(score ~ education_level, error = model)
```
The simple main effect of “education_level” on job satisfaction score was statistically significant for both male and female (p < 0.0001).

In other words, there is a statistically significant difference in mean job satisfaction score between males educated to either school, college or university level, F(2, 52) = 132, p < 0.0001. The same conclusion holds true for females, F(2, 52) = 62.8, p < 0.0001.

You can run and interpret all possible pairwise comparisons using a Bonferroni adjustment. This can be easily done using the function emmeans_test() [rstatix package], a wrapper around the emmeans package, which needs to be installed. Emmeans stands for estimated marginal means (aka least square means or adjusted means).

Compare the score of the different education levels by gender levels:

``` {r}
pwc <- jobsatisfaction %>% 
  group_by(gender) %>%
  emmeans_test(score ~ education_level, p.adjust.method = "bonferroni") 
pwc
```

There was a significant difference of job satisfaction score between all groups for both males and females (p < 0.05).

### Procedure for non-significant two-way interaction

If the two-way interaction is not statistically significant, you need to consult the main effect for each of the two variables (gender and education_level) in the ANOVA output.

#### Pairwise comparisons

A statistically significant simple main effect can be followed up by multiple pairwise comparisons to determine which group means are different. We’ll now perform multiple pairwise comparisons between the different education_level groups by gender.

```{r}
pairwise <- jobsatisfaction %>%
  pairwise_t_test(
    score ~ education_level, 
    p.adjust.method = "bonferroni"
    )
pairwise
```

## Summary 

Now, let's summarize the ANOVA test using a plot together with the labeled results and footers.

```{r}
pwc <- pwc %>% add_xy_position(x="gender")
bxp + 
  stat_pvalue_manual(pwc) +
  labs(
    subtitle=get_test_label(results2,detailed=TRUE),
    caption=get_pwc_label(pwc)
  )

```

A two-way ANOVA was conducted to examine the effects of gender and education level on job satisfaction score.

Residual analysis was performed to test for the assumptions of the two-way ANOVA. Outliers were assessed by box plot method, normality was assessed using Shapiro-Wilk’s normality test and homogeneity of variances was assessed by Levene’s test.

There were no extreme outliers, residuals were normally distributed (p > 0.05) and there was homogeneity of variances (p > 0.05).

There was a statistically significant interaction between gender and education level on job satisfaction score, F(2, 52) = 7.33, p = 0.0016, eta2[g] = 0.22.

Consequently, an analysis of simple main effects for education level was performed with statistical significance receiving a Bonferroni adjustment. There was a statistically significant difference in mean “job satisfaction” scores for both males (F(2, 52) = 132, p < 0.0001) and females (F(2, 52) = 62.8, p < 0.0001) educated to either school, college or university level.

All pairwise comparisons were analyzed between the different education_level groups organized by gender. There was a significant difference of Job Satisfaction score between all groups for both males and females (p < 0.05)


## YOUR TURN

The data comes from [https://flixgem.com/](https://flixgem.com/) (dataset version as of March 12, 2021). The data contains information on 9425 movies and series available on Netlix.

Please check if the IMDB scores are significantly different for different movie/series view ratings and genres.

``` {r challenge}
library(readr)
knitr::opts_chunk$set(echo = TRUE)
download.file("https://raw.githubusercontent.com/kflisikowski/ds/master/netflix-dataset.csv?raw=true", destfile ="dane.csv",mode="wb")
dane<-read.csv(file="dane.csv",encoding ="UTF-8",header=TRUE,sep = ",")
attach(dane)

dane %>%
  na.omit() %>%
  shapiro.test(IMDb.Score)

dane %>%
  group_by(View.Rating, Genre) %>%
  shapiro.test(IMDb.Score)
```
##identify outlayers
```{r}
dane %>%
  filter(Languages %in% c("Polish")) %>%
  ggqqplot("IMDb.Score")+
  facet_grid(View.Rating~Genre)

dane %>%
  max(IMDb.Votes)
dane %>%
  na.omit()%>%
dane1<- dane %>% na.omit()
  max(dane1$IMDb.Votes)
```
```{r}
dane %>%
  filter(Languages %in% c("Polish")) %>%
  levene_test(IMDb.Score~View.Rating*Genre)



```

```{r}
pairwise <- dane %>%
  pairwise_t_test(
    IMDb.Score ~ Genre,
    p.adjust.method = "bonferroni"
  )
pairwise

```
